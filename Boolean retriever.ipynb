{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from tqdm import tqdm\n",
    "\n",
    "ps = PorterStemmer()\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e\n"
     ]
    }
   ],
   "source": [
    "print('e')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvertedIndex:\n",
    "    def __init__(self, directory, stopwords, save=True, name='ii'):\n",
    "        self.directory = directory\n",
    "        self.stopwords = stopwords\n",
    "        self.save = save\n",
    "        self.name = name\n",
    "        self.id_to_file = {}\n",
    "        self.index = defaultdict(lambda: {'count': [], 'words': set()}) # stemmed index\n",
    "        self.windex = defaultdict(lambda: {'count': [], 'postings': set(), 'rotations':set()}) # word index\n",
    "        self.tgi = defaultdict(lambda: set())\n",
    "        self.construct()\n",
    "        self.construct_tgi()\n",
    "        \n",
    "    def produce_rotations(self, word):\n",
    "        term = \"$\" + word\n",
    "        res = [term]\n",
    "        for i in range(len(word) - 1):\n",
    "            term = term[-1] + term[:-1]\n",
    "            res.append(term)\n",
    "        return res\n",
    "    \n",
    "    def construct(self):\n",
    "        for i, filename in tqdm(enumerate(os.listdir(self.directory))):\n",
    "            self.id_to_file[i] = filename\n",
    "            with open(os.path.join(self.directory, filename), 'rt') as original:\n",
    "                sents = sent_tokenize(original.read())\n",
    "                for s in sents:\n",
    "                    for w in word_tokenize(s):\n",
    "                        if re.match(\"^[-'a-zA-Z]+$\", w): # if it is a proper term\n",
    "                            w = w.lower()\n",
    "                            stemmed = ps.stem(w)\n",
    "                            if stemmed not in self.stopwords:\n",
    "                                    self.index[stemmed]['words'].add(w)\n",
    "                                    self.windex[w]['postings'].add(i)\n",
    "                            \n",
    "        \n",
    "        for t in self.index.keys():\n",
    "            postings = set()\n",
    "\n",
    "            for w in self.index[t]['words']:\n",
    "                self.windex[w]['count'] = len(self.windex[w]['postings'])\n",
    "                postings|=set(self.windex[w]['postings'])\n",
    "                self.windex[w]['rotations'] = set(self.produce_rotations(w))\n",
    "            self.index[t]['count'] = len(postings)\n",
    "            \n",
    "        if self.save:\n",
    "            np.save(self.name, np.array(dict(self.index)))\n",
    "\n",
    "    def construct_tgi(self):\n",
    "        for i in self.index.keys():\n",
    "            for j in self.index[i]['words']:\n",
    "                for k in range(len(j) - 1):\n",
    "                    self.tgi[j[k:k+2]].add(i) # storing stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "42it [01:13,  1.76s/it]\n"
     ]
    }
   ],
   "source": [
    "ii = InvertedIndex(directory = 'Datasets/Shakespeare', stopwords = stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'count': 17, 'words': {'whore', 'whored', 'whores', 'whoring'}}"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ii.index['whore']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'count': 1,\n",
       " 'postings': {22},\n",
       " 'rotations': {'$whoring',\n",
       "  'g$whorin',\n",
       "  'horing$w',\n",
       "  'ing$whor',\n",
       "  'ng$whori',\n",
       "  'oring$wh',\n",
       "  'ring$who'}}"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ii.windex['whoring']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryHandler:\n",
    "    def __init__(self):\n",
    "        self.symbols = {}\n",
    "    \n",
    "    def rotate(self, wildcard):\n",
    "        term = '$' + wildcard\n",
    "        for i, l in enumerate(term, 1):\n",
    "            if l == \"*\":\n",
    "                return term[i:] + term[:i-1], True\n",
    "        else:\n",
    "            return wildcard, False\n",
    "        \n",
    "    def union(self, p1, p2):\n",
    "        res = set()\n",
    "        res = (set(p1) | set(p2))\n",
    "        return list(res)\n",
    "\n",
    "    def inverse(self, p1, total):\n",
    "        return [i for i in total if i not in p1]\n",
    "    \n",
    "    def intersection(self, p1, p2):\n",
    "        res = set()\n",
    "        res = (set(p1) & set(p2))\n",
    "        return list(res)\n",
    "    \n",
    "    def and_not(self, p1, p2):\n",
    "        i = j = 0\n",
    "        res = []\n",
    "\n",
    "        while i < len(p1) and j < len(p2):\n",
    "            if p1[i] == p2[j]:\n",
    "                i += 1\n",
    "                j += 1\n",
    "            elif p1[i] < p2[j]:\n",
    "                res.append(p1[i])\n",
    "                i += 1\n",
    "            elif p1[i] > p2[j]:\n",
    "                j += 1\n",
    "        if i < len(p1):\n",
    "            res += p1[i:]\n",
    "\n",
    "        return res\n",
    "    \n",
    "    def or_not(self, p1, p2, total):    \n",
    "        return self.union(p1, self.inverse(p2, total))\n",
    "    \n",
    "    def levenshtein_distance(self, word1, word2):\n",
    "        m = np.zeros((len(word1)+1, len(word2)+1))\n",
    "        for j in range(len(word1)+1):\n",
    "            m[0][j] = j\n",
    "        for i in range(len(word2)+1):\n",
    "            m[i][0] = i\n",
    "\n",
    "        for i in range(1, len(word1)+1):\n",
    "            for j in range(1, len(word2)+1):\n",
    "                if word1[i-1] == word2[j-1]:\n",
    "                    m[i, j] = m[i-1, j-1]\n",
    "                else:\n",
    "                    m[i, j] = 1 + min(m[i-1, j], min(m[i, j-1], m[i-1, j-1]))\n",
    "        return m[len(word1), len(word2)]\n",
    "    \n",
    "    def spell_correct(self, misspelled, ii):\n",
    "        twograms = []\n",
    "        for i in range(len(misspelled) - 1):\n",
    "            twograms += ii.tgi[misspelled[i:i+2]]\n",
    "        \n",
    "        freqs = dict(collections.Counter(twograms)) # stem : no. of matching two-grams\n",
    "        freqs = {k: v for k, v in reversed(sorted(freqs.items(), key=lambda item: item[1]))}\n",
    "        # print(freqs)\n",
    "        \n",
    "        ff = defaultdict(lambda: []) # no.of matching two-grams: stem\n",
    "        \n",
    "        for k, v in freqs.items():\n",
    "            ff[v].append(k)\n",
    "        # print(ff)\n",
    "            \n",
    "        ed = defaultdict(lambda: set())\n",
    "        \n",
    "        for f in list(ff.keys())[:1]: # top two-gram matching word\n",
    "            for i in ff[f]: # for each stem with frequency f\n",
    "                for w in ii.index[i]['words']: # for each word in that stem\n",
    "                    if len(w)>=len(misspelled)-4 and len(w)<=len(misspelled)+4 : # if at most 4 chars away from misspelled word\n",
    "                        d = self.levenshtein_distance(misspelled, w) # get distance\n",
    "                        if d<=5: # if dist at most 5\n",
    "                            ed[d].add(i) # add stem\n",
    "        # print(ed)\n",
    "\n",
    "        if not ed:\n",
    "            return \"\"\n",
    "        return max([(ii.index[x]['count'], x) for x in ed[min(list(ed.keys()))]])[1]\n",
    "        \n",
    "    \n",
    "    def match(self, term, ii):\n",
    "        if term[0] == '@':\n",
    "            return self.symbols[term]\n",
    "        res = []\n",
    "        rotated, is_wild = self.rotate(term)\n",
    "        #print(rotated)\n",
    "        if is_wild: # is a wildcard\n",
    "            for i in ii.index.keys():\n",
    "                for w in ii.index[i]['words']:\n",
    "                    if len(w) >= len(term)-1:\n",
    "                        for r in ii.windex[w]['rotations']:\n",
    "                            if r[:len(rotated)] == rotated:\n",
    "                                #print(r)\n",
    "                                #print(w)\n",
    "                                #print(ii.windex[w]['postings'])\n",
    "                                res = self.union(res, ii.windex[w]['postings'])\n",
    "                                break\n",
    "        else: # not a wildcard\n",
    "            rotated = ps.stem(rotated)\n",
    "            for i in ii.index.keys():\n",
    "                if i == rotated:\n",
    "                    for w in ii.index[i]['words']:\n",
    "                        res = set(res)\n",
    "                        res|= set(ii.windex[w]['postings'])\n",
    "                    break\n",
    "                    \n",
    "        if not is_wild and not res: # misspelled word\n",
    "            corrected = self.spell_correct(term, ii)\n",
    "            print(term + \" is corrected to \" + corrected)\n",
    "            if corrected:\n",
    "                return self.match(corrected, ii)\n",
    "        \n",
    "        return list(res)\n",
    "    \n",
    "    def evaluate_expr(self, expr, i, ii, total):\n",
    "        print(\"evaluating \" + expr + \" and storing as @\" + str(i))\n",
    "        # var or not var\n",
    "        # var or var\n",
    "        # var and not var\n",
    "        # var and var\n",
    "        # var\n",
    "        # not var\n",
    "\n",
    "        keywords = [\"and\", \"or\", \"not\"]\n",
    "        expr = expr.split(\" \")\n",
    "        new_symbol = '@' + str(i)\n",
    "\n",
    "        if expr[0] == \"not\":\n",
    "            self.symbols[new_symbol] = self.inverse(self.match(expr[1], ii), total)\n",
    "            return new_symbol\n",
    "\n",
    "        else:\n",
    "            if len(expr) == 1:\n",
    "                self.symbols[new_symbol] = self.match(expr[0], ii)\n",
    "                return new_symbol\n",
    "\n",
    "            if expr[1] == 'and':\n",
    "                if expr[2] == 'not':\n",
    "                    self.symbols[new_symbol] = self.and_not(self.match(expr[0], ii), self.match(expr[3], ii))\n",
    "                    return new_symbol\n",
    "\n",
    "                else:\n",
    "                    self.symbols[new_symbol] = self.intersection(self.match(expr[0], ii), self.match(expr[2], ii))\n",
    "                    return new_symbol\n",
    "\n",
    "            else:\n",
    "                if expr[2] == 'not':\n",
    "                    self.symbols[new_symbol] = self.or_not(self.match(expr[0], ii), self.match(expr[3], ii), total)\n",
    "                    return new_symbol\n",
    "\n",
    "                else:\n",
    "                    self.symbols[new_symbol] = self.union(self.match(expr[0], ii), self.match(expr[2], ii))\n",
    "                    return new_symbol\n",
    "            \n",
    "    def compute(self, query, ii, total):\n",
    "        stack = []\n",
    "        self.symbols = {}\n",
    "        i = 0\n",
    "        for c in query:\n",
    "            if c != ')':\n",
    "                stack.append(c)\n",
    "            else:\n",
    "                expr = \"\"\n",
    "                while stack:\n",
    "                    char = stack.pop()\n",
    "                    if char != '(':\n",
    "                        expr += char\n",
    "                    else:\n",
    "                        stack += list(self.evaluate_expr(expr[::-1], i, ii, total))\n",
    "                        i += 1\n",
    "                        break\n",
    "        if stack:\n",
    "            self.evaluate_expr(\"\".join(stack), i, ii, total)\n",
    "            i += 1\n",
    "        return self.symbols['@' + str(i - 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ii.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating man* and storing as @0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"man*\"\n",
    "qh = QueryHandler()\n",
    "qh.compute(query, ii, list(ii.id_to_file.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qh.levenshtein_distance('whorr', 'short')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'count': 15,\n",
       " 'postings': [2, 5, 6, 8, 13, 16, 19, 20, 22, 26, 30, 35, 37, 38, 39],\n",
       " 'rotations': ['$whore', 'e$whor', 're$who', 'ore$wh', 'hore$w']}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ii.windex[\"whore\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ld(word1, word2):\n",
    "        m = np.zeros((len(word1)+1, len(word2)+1))\n",
    "\n",
    "        for i in range(len(word1)+1):\n",
    "            for j in range(len(word2)+1):\n",
    "                if i==0:\n",
    "                    m[i][j] = j\n",
    "                if j==0:\n",
    "                    m[i][j] = i\n",
    "                else:\n",
    "                    if word1[i-1] == word2[j-1]:\n",
    "                        m[i, j] = m[i-1, j-1]\n",
    "                    else:\n",
    "                        m[i, j] = 1 + min(m[i-1, j], min(m[i, j-1], m[i-1, j-1]))\n",
    "        return m[len(word1), len(word2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = ld('whorr', 'short')\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. 1. 0. 1.]\n",
      " [1. 1. 2. 2. 1. 1.]\n",
      " [1. 2. 1. 2. 2. 2.]\n",
      " [1. 2. 2. 1. 2. 3.]\n",
      " [1. 2. 3. 2. 1. 2.]\n",
      " [1. 2. 3. 3. 2. 2.]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
